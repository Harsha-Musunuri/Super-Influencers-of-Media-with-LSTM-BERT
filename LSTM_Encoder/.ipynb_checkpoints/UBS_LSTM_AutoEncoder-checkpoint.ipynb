{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed,LSTM, Activation, RepeatVector, Bidirectional,Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../../News_Category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description        date  \n",
       "0  She left her husband. He killed their children...  2018-05-26  \n",
       "1                           Of course it has a song.  2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ...  2018-05-26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = data['headline'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200853"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=random.shuffle(title, random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=titles[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14588 Words.\n",
      "4097 unique words.\n",
      "10 Most common words in the titles:\n",
      "\"New\" \"The\" \"York\" \"Times\" \"-\" \"to\" \"in\" \"a\" \"of\" \"the\" \"Trump\" \"and\" \"for\" \"on\" \"Is\" \"With\" \"Trumpâ€™s\" \"at\" \"as\" \"by\"\n"
     ]
    }
   ],
   "source": [
    "titles_counter = collections.Counter([word for sentence in titles for word in sentence.split()])\n",
    "\n",
    "print('{} Words.'.format(len([word for sentence in titles for word in sentence.split()])))\n",
    "print('{} unique words.'.format(len(titles_counter)))\n",
    "print('10 Most common words in the titles:')\n",
    "print('\"' + '\" \"'.join(list(zip(*titles_counter.most_common(20)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        # Find the length of the longest sequence/sentence\n",
    "        length = max([len(seq) for seq in x])\n",
    "    \n",
    "    return pad_sequences(sequences=x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max Title length: 21\n",
      "Title vocabulary size: 3696\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "\n",
    "    return preprocess_x, x_tk\n",
    "\n",
    "preproc_titles, titles_tokenizer = preprocess(titles)\n",
    "    \n",
    "max_title_sequence_length = preproc_titles.shape[1]\n",
    "\n",
    "titles_vocab_size = len(titles_tokenizer.word_index)\n",
    "\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max Title length:\", max_title_sequence_length)\n",
    "print(\"Title vocabulary size:\", titles_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_titles[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.expand_dims(preproc_titles, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperParams\n",
    "num_words = 3700\n",
    "maxlen = 21\n",
    "embed_dim = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(maxlen,), name='Encoder-Input')\n",
    "emb_layer = Embedding(num_words, embed_dim,input_length = maxlen, name='Body-Word-Embedding', mask_zero=False)\n",
    "x = emb_layer(encoder_inputs)\n",
    "state_h = Bidirectional(LSTM(128, activation='relu', name='Encoder-Last-LSTM'))(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = RepeatVector(maxlen)(seq2seq_encoder_out)\n",
    "decoder_lstm = Bidirectional(LSTM(128, return_sequences=True, name='Decoder-LSTM-before'))\n",
    "decoder_lstm_output = decoder_lstm(decoded)\n",
    "decoder_dense = Dense(num_words, activation='softmax', name='Final-Output-Dense-before')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 7s 117ms/step - loss: 5.4832\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 4.7808\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 9s 137ms/step - loss: 4.7526\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 8s 129ms/step - loss: 4.7369\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 8s 120ms/step - loss: 4.7256\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 9s 135ms/step - loss: 4.7150\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 8s 125ms/step - loss: 4.7076\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 7s 118ms/step - loss: 4.6991\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 8s 127ms/step - loss: 4.6920\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 9s 141ms/step - loss: 4.6864\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model = Model(encoder_inputs, decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "history = seq2seq_Model.fit(preproc_titles, np.expand_dims(preproc_titles, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Corona is bitch\"]\n",
    "seq,seq_tokenizer = tokenize(sentences)\n",
    "pad_seq=pad_sequences(seq, maxlen=21, padding='post')\n",
    "sentence_vec = encoder_model.predict(pad_seq)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5400781e+03, 2.1195332e+03, 1.9452211e+03, 1.4468572e+03,\n",
       "       2.7062571e+03, 1.6349536e+03, 2.0579385e+03, 1.5932351e+03,\n",
       "       2.4487239e+03, 1.9050825e+03, 2.0784609e+00, 2.1536016e+03,\n",
       "       1.6867900e+03, 0.0000000e+00, 1.9955588e+03, 3.0597051e+03,\n",
       "       5.0063408e-04, 0.0000000e+00, 1.7180950e+03, 1.8202684e+03,\n",
       "       2.0634824e+03, 9.0168103e+02, 3.8030848e-01, 1.0607266e+03,\n",
       "       2.8046865e+03, 2.2180254e+03, 2.0974800e+03, 1.9343760e+03,\n",
       "       0.0000000e+00, 2.2577852e+03, 3.3668691e+03, 1.3126469e+03,\n",
       "       1.8436913e+03, 0.0000000e+00, 7.0035736e+02, 1.2686010e+03,\n",
       "       1.7521382e+03, 2.5970186e+03, 3.8825687e+02, 1.7115757e+03,\n",
       "       2.7330707e+02, 2.0190470e+03, 0.0000000e+00, 2.0208607e+03,\n",
       "       2.3512793e+03, 1.9183210e+03, 2.4099807e+03, 0.0000000e+00,\n",
       "       3.5299006e+03, 5.8409735e+02, 0.0000000e+00, 9.5661932e+02,\n",
       "       8.4618499e-05, 2.2348738e+03, 2.7969570e+03, 2.7983208e+03,\n",
       "       1.9414551e+03, 0.0000000e+00, 6.0455139e+02, 4.8234296e+02,\n",
       "       1.6219529e+03, 0.0000000e+00, 2.5743674e+03, 2.5260498e+03,\n",
       "       1.9857509e+03, 2.2456641e+03, 2.5646335e+03, 3.2134946e+03,\n",
       "       3.0166902e+03, 1.0979611e+03, 1.3384441e+03, 1.8603364e+03,\n",
       "       8.8676404e+02, 1.8082920e+03, 2.2057603e+03, 1.4917449e+03,\n",
       "       2.6508721e+03, 0.0000000e+00, 1.5121238e+03, 0.0000000e+00,\n",
       "       3.0377246e+03, 6.1699213e+02, 0.0000000e+00, 2.4046587e+03,\n",
       "       1.6866484e+03, 2.6712749e+03, 7.5850354e+02, 8.9756250e+02,\n",
       "       9.7857660e+02, 1.3608774e+03, 2.2087249e+03, 2.5762261e+03,\n",
       "       1.3134756e+00, 0.0000000e+00, 1.3421030e+03, 8.5262016e+01,\n",
       "       2.0185502e+03, 2.0308005e+03, 1.7649048e+03, 1.3865510e+03,\n",
       "       1.0694763e+01, 6.9532947e+02, 2.1029099e+03, 1.8805637e+03,\n",
       "       1.9007639e+03, 1.2974312e+03, 1.4291080e+03, 2.5509363e+03,\n",
       "       2.0614827e+03, 2.7337773e+03, 2.2551692e+03, 1.6217542e+03,\n",
       "       1.2327711e+03, 2.1872439e+00, 7.0382153e+02, 1.9664996e+03,\n",
       "       2.5407205e+03, 1.5368564e+03, 9.0257349e+02, 2.3560815e+03,\n",
       "       1.9280256e+03, 0.0000000e+00, 2.8403809e+03, 1.0341245e+03,\n",
       "       3.2300024e+03, 1.4081223e+03, 2.8514893e+00, 1.1642389e+00,\n",
       "       5.5986816e-01, 1.0175532e+00, 1.5105983e+00, 3.5522816e+00,\n",
       "       2.8571239e+00, 2.2357800e+00, 2.3004825e+00, 6.4454174e-01,\n",
       "       1.3123280e+00, 6.1937320e-01, 5.5857449e+00, 8.4738123e-01,\n",
       "       4.1736679e+00, 2.9017012e+00, 2.8506775e+00, 0.0000000e+00,\n",
       "       1.1406038e+00, 3.1549578e+00, 2.0538212e-01, 3.2481647e+00,\n",
       "       2.1475074e+00, 4.0269117e+00, 1.9469732e+00, 1.8597572e+00,\n",
       "       2.3103027e+00, 1.1013643e+00, 1.9114186e+00, 1.4596683e+00,\n",
       "       3.4624043e+00, 1.0693097e+00, 3.6828079e+00, 1.4697005e+00,\n",
       "       2.6760659e+00, 2.0147111e-02, 2.7971557e-01, 1.7936243e+00,\n",
       "       2.9178729e+00, 3.9535868e-01, 1.3699231e+00, 3.2676730e+00,\n",
       "       5.9832203e-01, 2.5601187e+00, 2.0899517e-02, 2.7945867e+00,\n",
       "       2.1028292e+00, 2.1269040e+00, 3.9077561e+00, 1.0375332e+00,\n",
       "       2.6599367e+00, 1.0474865e+00, 1.9619921e+00, 8.7507975e-01,\n",
       "       1.6499194e+00, 2.0703218e+00, 4.2205834e+00, 2.9101486e+00,\n",
       "       6.8545759e-01, 1.4291614e+00, 8.9182746e-01, 8.0840713e-01,\n",
       "       1.4293618e+00, 3.6824598e+00, 9.1299230e-01, 1.2074215e+00,\n",
       "       1.4106070e+00, 2.2865644e+00, 2.7258642e+00, 4.6527016e-01,\n",
       "       1.4621764e+00, 1.1468506e+00, 1.2320659e+00, 4.2924862e+00,\n",
       "       1.1955894e-01, 5.1715827e+00, 3.5850849e+00, 0.0000000e+00,\n",
       "       2.8948885e-01, 4.0226194e-01, 4.6128446e-01, 7.3358917e-01,\n",
       "       2.0508888e+00, 2.3404047e+00, 7.9283547e-01, 1.8208003e+00,\n",
       "       4.2511024e+00, 2.0729642e+00, 2.7502418e+00, 2.5684567e-02,\n",
       "       2.4586804e+00, 4.3564615e+00, 4.7843738e+00, 4.1608577e+00,\n",
       "       8.1043929e-02, 3.4474411e+00, 1.2812945e+00, 2.6524020e-05,\n",
       "       1.6236230e+00, 3.2770751e+00, 2.3901970e-04, 2.1723325e+00,\n",
       "       2.7063429e+00, 2.2394083e+00, 1.4820023e+00, 5.7686103e-04,\n",
       "       3.3345255e-01, 3.3923805e+00, 1.6165106e+00, 2.3013058e+00,\n",
       "       9.5467395e-01, 6.5402424e-01, 1.4182781e+00, 3.6526513e-01,\n",
       "       2.5005435e-04, 3.2775357e+00, 3.6680536e+00, 5.4741225e+00,\n",
       "       1.2839890e+00, 1.1456238e+00, 9.0324157e-01, 1.5270984e+00,\n",
       "       4.1143270e+00, 4.5595889e+00, 9.4393736e-01, 1.4452517e+00,\n",
       "       1.4776095e+00, 1.0862733e+00, 4.6814966e-01, 2.5389454e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
