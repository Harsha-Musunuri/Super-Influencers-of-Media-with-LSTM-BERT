{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed,LSTM, Activation, RepeatVector, Bidirectional,Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"News_Category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description        date  \n",
       "0  She left her husband. He killed their children...  2018-05-26  \n",
       "1                           Of course it has a song.  2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ...  2018-05-26  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = data['headline'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200853"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laura Linney Gives Birth to a Baby Boy at Age 49 and the Question Remains, Should You Ever Give Up Your Dream to Conceive?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=titles[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191011 Words.\n",
      "31724 unique words.\n",
      "10 Most common words in the titles:\n",
      "\"The\" \"To\" \"A\" \"In\" \"Of\" \"For\" \"Is\" \"And\" \"On\" \"the\" \"With\" \"to\" \"Your\" \"You\" \"How\" \"of\" \"Trump\" \"New\" \"(PHOTOS)\" \"This\"\n"
     ]
    }
   ],
   "source": [
    "titles_counter = collections.Counter([word for sentence in titles for word in sentence.split()])\n",
    "\n",
    "print('{} Words.'.format(len([word for sentence in titles for word in sentence.split()])))\n",
    "print('{} unique words.'.format(len(titles_counter)))\n",
    "print('10 Most common words in the titles:')\n",
    "print('\"' + '\" \"'.join(list(zip(*titles_counter.most_common(20)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        # Find the length of the longest sequence/sentence\n",
    "        length = max([len(seq) for seq in x])\n",
    "    \n",
    "    return pad_sequences(sequences=x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max Title length: 33\n",
      "Title vocabulary size: 24526\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "\n",
    "    return preprocess_x, x_tk\n",
    "\n",
    "preproc_titles, titles_tokenizer = preprocess(titles)\n",
    "    \n",
    "max_title_sequence_length = preproc_titles.shape[1]\n",
    "\n",
    "titles_vocab_size = len(titles_tokenizer.word_index)\n",
    "\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max Title length:\", max_title_sequence_length)\n",
    "print(\"Title vocabulary size:\", titles_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_titles[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.expand_dims(preproc_titles, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperParams\n",
    "num_words = 25000\n",
    "maxlen = 53\n",
    "embed_dim = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(maxlen,), name='Encoder-Input')\n",
    "emb_layer = Embedding(num_words, embed_dim,input_length = maxlen, name='Body-Word-Embedding', mask_zero=False)\n",
    "x = emb_layer(encoder_inputs)\n",
    "state_h = Bidirectional(LSTM(128, activation='relu', name='Encoder-Last-LSTM'),merge_mode='sum')(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = RepeatVector(maxlen)(seq2seq_encoder_out)\n",
    "decoder_lstm = Bidirectional(LSTM(128, return_sequences=True, name='Decoder-LSTM-before'),merge_mode='sum')\n",
    "decoder_lstm_output = decoder_lstm(decoded)\n",
    "decoder_dense = Dense(num_words, activation='softmax', name='Final-Output-Dense-before')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 398s 20ms/sample - loss: 2.0025\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 390s 19ms/sample - loss: 1.9758\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 385s 19ms/sample - loss: 1.9691\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 390s 19ms/sample - loss: 1.9650\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 391s 20ms/sample - loss: 1.9611\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 389s 19ms/sample - loss: 1.9582\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 391s 20ms/sample - loss: 1.9553\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 389s 19ms/sample - loss: 1.9526\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 390s 19ms/sample - loss: 1.9498\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 391s 20ms/sample - loss: 1.9476\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "seq2seq_Model = Model(encoder_inputs, decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "history = seq2seq_Model.fit(preproc_titles, np.expand_dims(preproc_titles, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Corona is bitch\"]\n",
    "seq,seq_tokenizer = tokenize(sentences)\n",
    "pad_seq=pad_sequences(seq, maxlen=53, padding='post')\n",
    "sentence_vec = encoder_model.predict(pad_seq)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7154497e+10, 2.3128285e+00, 1.8948966e+10, 0.0000000e+00,\n",
       "       2.4477364e+10, 6.9981921e+09, 9.0288293e+02, 1.8066031e+10,\n",
       "       0.0000000e+00, 8.0409114e+09, 3.0993208e+03, 1.2019243e-10,\n",
       "       2.4211485e+10, 1.2311431e+10, 2.6360735e+03, 9.5063368e+09,\n",
       "       2.6514760e-01, 1.6312970e+10, 1.3306758e+10, 8.8617349e+09,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.8690505e+10, 2.4993655e+10,\n",
       "       0.0000000e+00, 0.0000000e+00, 2.7861638e+03, 1.1276770e+03,\n",
       "       3.7062963e+09, 1.6781406e+10, 1.8797748e+10, 2.4847573e+10,\n",
       "       1.1200934e+10, 4.8212398e+09, 3.1813740e+03, 2.2980510e+10,\n",
       "       1.7623846e+03, 0.0000000e+00, 6.9290004e+09, 3.3853710e+10,\n",
       "       1.9593732e+10, 2.1315430e+03, 3.0775521e+09, 0.0000000e+00,\n",
       "       1.2079585e+10, 5.9122246e+08, 0.0000000e+00, 2.3526877e+10,\n",
       "       6.3832576e+09, 2.0302156e+10, 1.8359994e+10, 1.7828168e+10,\n",
       "       9.4540595e+09, 1.2882727e+10, 6.5908582e+09, 2.9732536e+10,\n",
       "       1.6303486e+10, 0.0000000e+00, 1.6761657e+00, 2.1479334e+10,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.9329819e+03, 2.6945839e+10,\n",
       "       2.4253985e+10, 1.5468135e+03, 3.0236214e+09, 2.9558618e+10,\n",
       "       0.0000000e+00, 1.3364929e+10, 1.1530266e+10, 8.8488477e+02,\n",
       "       1.4797630e+10, 8.0866765e+09, 2.3961534e+10, 1.5754341e+10,\n",
       "       2.4666534e+10, 1.5762259e+10, 0.0000000e+00, 1.9223235e+10,\n",
       "       1.0360373e+10, 7.6544169e+09, 1.8185007e+10, 1.6461390e+10,\n",
       "       3.1011245e+03, 3.3131321e+03, 1.4935579e+10, 1.1387258e+10,\n",
       "       2.6275205e+03, 0.0000000e+00, 1.6506204e+10, 4.2121329e+02,\n",
       "       4.3187587e-09, 8.1077786e+09, 1.4627495e+10, 1.6985196e+10,\n",
       "       4.6843822e+09, 9.2168806e+09, 2.5298676e+10, 7.2031268e+09,\n",
       "       1.3611657e+10, 6.8087392e+00, 1.3483632e+03, 0.0000000e+00,\n",
       "       2.8204134e+10, 1.1474972e+09, 5.5015931e+09, 8.6638981e+09,\n",
       "       1.9979084e+10, 0.0000000e+00, 4.8585556e+07, 3.9562429e+09,\n",
       "       1.7500262e+10, 5.6192917e-10, 2.3768091e+00, 9.4482606e+09,\n",
       "       2.9168660e+09, 2.0392686e+10, 2.7287615e+10, 1.2322777e+10,\n",
       "       0.0000000e+00, 2.1803055e+10, 1.3489241e+10, 1.8090160e+10,\n",
       "       1.9920648e+09, 1.4775435e+10, 3.3336736e+10, 6.2084244e+09],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model.save('saveModel/LSTM_EncoderDecoderTrained') \n",
    "encoder_model.save('saveModel/LSTM_EncoderTrained') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "Encoder-Model (Model)        (None, 128)               1433296   \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 53, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 53, 128)           263168    \n",
      "_________________________________________________________________\n",
      "Final-Output-Dense-before (D (None, 53, 25000)         3225000   \n",
      "=================================================================\n",
      "Total params: 4,921,464\n",
      "Trainable params: 4,921,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model = tf.keras.models.load_model('saveModel/LSTM_EncoderDecoderTrained')\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Model: \"Encoder-Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   [(None, 53)]              0         \n",
      "_________________________________________________________________\n",
      "Body-Word-Embedding (Embeddi (None, 53, 50)            1250000   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 128)               183296    \n",
      "=================================================================\n",
      "Total params: 1,433,296\n",
      "Trainable params: 1,433,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = tf.keras.models.load_model('saveModel/LSTM_EncoderTrained')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7154497e+10, 2.3128285e+00, 1.8948966e+10, 0.0000000e+00,\n",
       "       2.4477364e+10, 6.9981921e+09, 9.0288293e+02, 1.8066031e+10,\n",
       "       0.0000000e+00, 8.0409114e+09, 3.0993208e+03, 1.2019243e-10,\n",
       "       2.4211485e+10, 1.2311431e+10, 2.6360735e+03, 9.5063368e+09,\n",
       "       2.6514760e-01, 1.6312970e+10, 1.3306758e+10, 8.8617349e+09,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.8690505e+10, 2.4993655e+10,\n",
       "       0.0000000e+00, 0.0000000e+00, 2.7861638e+03, 1.1276770e+03,\n",
       "       3.7062963e+09, 1.6781406e+10, 1.8797748e+10, 2.4847573e+10,\n",
       "       1.1200934e+10, 4.8212398e+09, 3.1813740e+03, 2.2980510e+10,\n",
       "       1.7623846e+03, 0.0000000e+00, 6.9290004e+09, 3.3853710e+10,\n",
       "       1.9593732e+10, 2.1315430e+03, 3.0775521e+09, 0.0000000e+00,\n",
       "       1.2079585e+10, 5.9122246e+08, 0.0000000e+00, 2.3526877e+10,\n",
       "       6.3832576e+09, 2.0302156e+10, 1.8359994e+10, 1.7828168e+10,\n",
       "       9.4540595e+09, 1.2882727e+10, 6.5908582e+09, 2.9732536e+10,\n",
       "       1.6303486e+10, 0.0000000e+00, 1.6761657e+00, 2.1479334e+10,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.9329819e+03, 2.6945839e+10,\n",
       "       2.4253985e+10, 1.5468135e+03, 3.0236214e+09, 2.9558618e+10,\n",
       "       0.0000000e+00, 1.3364929e+10, 1.1530266e+10, 8.8488477e+02,\n",
       "       1.4797630e+10, 8.0866765e+09, 2.3961534e+10, 1.5754341e+10,\n",
       "       2.4666534e+10, 1.5762259e+10, 0.0000000e+00, 1.9223235e+10,\n",
       "       1.0360373e+10, 7.6544169e+09, 1.8185007e+10, 1.6461390e+10,\n",
       "       3.1011245e+03, 3.3131321e+03, 1.4935579e+10, 1.1387258e+10,\n",
       "       2.6275205e+03, 0.0000000e+00, 1.6506204e+10, 4.2121329e+02,\n",
       "       4.3187587e-09, 8.1077786e+09, 1.4627495e+10, 1.6985196e+10,\n",
       "       4.6843822e+09, 9.2168806e+09, 2.5298676e+10, 7.2031268e+09,\n",
       "       1.3611657e+10, 6.8087392e+00, 1.3483632e+03, 0.0000000e+00,\n",
       "       2.8204134e+10, 1.1474972e+09, 5.5015931e+09, 8.6638981e+09,\n",
       "       1.9979084e+10, 0.0000000e+00, 4.8585556e+07, 3.9562429e+09,\n",
       "       1.7500262e+10, 5.6192917e-10, 2.3768091e+00, 9.4482606e+09,\n",
       "       2.9168660e+09, 2.0392686e+10, 2.7287615e+10, 1.2322777e+10,\n",
       "       0.0000000e+00, 2.1803055e+10, 1.3489241e+10, 1.8090160e+10,\n",
       "       1.9920648e+09, 1.4775435e+10, 3.3336736e+10, 6.2084244e+09],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Corona is bitch\"]\n",
    "seq,seq_tokenizer = tokenize(sentences)\n",
    "pad_seq=pad_sequences(seq, maxlen=53, padding='post')\n",
    "sentence_vec = encoder.predict(pad_seq)[0]\n",
    "sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Articles_100=pd.read_csv('10Category10Articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles_100 = Articles_100['headline'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Titles_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Titles_100[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latents for the 10 Articles of 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleLatents=[]\n",
    "for sentence in Titles_100:\n",
    "    seq,seq_tokenizer = tokenize(sentences)\n",
    "    pad_seq=pad_sequences(seq, maxlen=53, padding='post')\n",
    "    sentence_vec = encoder.predict(pad_seq)[0]\n",
    "    ArticleLatents.append(sentence_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ArticleLatents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary mapping between article to latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article2Latent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapArticle2Latent(Articles, Latents):\n",
    "    for i in range(len(Articles)):\n",
    "        Article2Latent[Articles[i]]=Latents[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapArticle2Latent(Titles_100,ArticleLatents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "LatentArray=np.stack( ArticleLatents, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 128)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatentArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/koko/system/anaconda/envs/python37/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py:1008: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  return self.fit(X, sample_weight=sample_weight).labels_\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(\n",
    "    n_clusters=10, init='random',\n",
    "    n_init=10, max_iter=300, \n",
    "    tol=0.001, random_state=0\n",
    ")\n",
    "Clusters = km.fit_predict(LatentArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster to Article Mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
